# Flash attention 1/2çš„æ—¶é—´ç©ºé—´å¤æ‚åº¦ç®€å•åˆ†æ
- [Flash Attention project](https://github.com/Dao-AILab/flash-attention)
- [Flash Attention 1 paper](https://arxiv.org/abs/2205.14135)
- [Flash Attention 2 paper](hhttps://arxiv.org/abs/2307.08691)
## Standard Attention
### FLOPS
- $Q_{NÃ—d}K^T_{dÃ—N}$ï¼šå°±æ˜¯ä¸€ä¸ªçŸ©é˜µä¹˜æ³•ï¼ŒçŸ©é˜µä¹˜æ³•$A_{MÃ—K}$ $B_{KÃ—N}$çš„`FLOPS`æ˜¯`MNK`ï¼Œæ‰€ä»¥è¿™é‡Œçš„`FLOPS`æ˜¯$O(N^2d)$
- $S_{NÃ—N}V_{NÃ—d}$ï¼š$O(N^2d)$
- æ‰€ä»¥æ€»çš„`FLOPS`æ˜¯ï¼š$O(N^2d)$
### Space
- å­˜å‚¨ä¸­é—´ç»“æœ`S`ï¼š$O(N^2)$
### IO(HBM)
- è¯»`Qã€Kã€V`ï¼š$O(Nd)$
- è¯»å†™ä¸­é—´å€¼`S`ï¼š$O(N^2)$
- å†™ç»“æœ`O`ï¼š$O(Nd)$
- æ‰€ä»¥æ€»çš„`IO`æ˜¯ï¼š$O(Nd+N^2d)$
![standard attention algorithm](imgs/standard-attention.png)
## Flash Attention 1
### FLOPS
- è™½ç„¶ç”¨äº†`tilling`åˆ†å—ï¼Œä½†æ˜¯`FLOPS`çš„å¤§å¤´è¿˜æ˜¯çŸ©é˜µä¹˜æ³•
- æ€»å…±`Tc * Tr`($\frac {N}{B_{c}} * \frac {N}{B_{r}}$)æ¬¡å°å—çš„è®¡ç®—
- æ¯ä¸ªå°å—çš„`FLOPS`ï¼š$O(B_{r}B_{c}d)$
- æ€»çš„`FLOPS`ï¼š$O(B_{r}B_{c} * d * T_{c} * T_{r})$ = $B_{r}B_{c} * d * \frac {N}{B_{c}} * \frac {N}{B_{r}}$ = $O(N^2d)$
### Space
- å¯¹äº`output`çš„æ¯ä¸€è¡Œï¼Œéœ€è¦é¢å¤–å­˜å‚¨`m`å’Œ`l`ï¼š$O(N)$
### IO(HMB)
- è¯»`Kã€V`ï¼š$O(Nd)$
- è¯»`Q`ï¼š$T_{c} * O(Nd)$ = $O(T_{c}Nd)$
- å†™`O`ï¼š$T_{c} * O(Nd)$ = $O(T_{c}Nd)$(`output`å’Œ`Q`çš„`shape`æ˜¯ä¸€æ ·çš„ï¼Œæ‰€ä»¥è¯»`Q`å’Œå†™`O`çš„`IO`æ¬¡æ•°æ˜¯ä¸€æ ·çš„)
- æ€»å…±çš„`IO`ï¼š$O(Nd) + O(2T_{c}Nd)$ = $O(Nd) + O(2\frac{N}{B_{c}}Nd)$
- $B_{c} = \frac{M}{4d}$
- æ‰€ä»¥ï¼š$O(Nd) + O(\frac{N}{B_{c}}Nd)$ = $O(Nd) + O(\frac{4dN}{M}Nd)$ = $O(Nd) + O(2\frac{4N^2d^2}{M})$
![Flash attention 1 algorithm](imgs/flash-attention-1.png)

### å¯¹æ¯”
- Standard Attentionï¼š$O(N^2d)$
- Flash Attention 1ï¼š$O(\frac{4N^2d^2}{M})$
- $\frac{Flash Attention 1}{Standard Attention}$ = $\frac{4d^2}{M} << 1$
> For typical values of ğ‘‘ (64-128) and ğ‘€ (around 100KB), ğ‘‘^2 is many times smaller than ğ‘€, and thus
FlashAttention requires many times fewer HBM accesses than standard implementation.

### Implement
- åœ¨`flash.cu`ä¸­çš„`myForward_kernel`
- æ¯ä¸ª`block`è´Ÿè´£ä¸€ä¸ªä¸€å°å—çš„ç»“æœï¼Œä¸€ä¸ªçº¿ç¨‹è´Ÿè´£ä¸€è¡Œçš„`Q`å’Œ`output`


## Flash Attention 2
- å¢åŠ `N`ç»´åº¦çš„å¹¶è¡Œ
- æ›´æ”¹`i`å’Œ`j`çš„å¾ªç¯æ¬¡åº
- å‡å°‘éçŸ©é˜µä¹˜æ³•çš„æ¬¡æ•°
### FLOPS
- ä¸å˜
### Space
- å¯¹äº`output`çš„æ¯ä¸€è¡Œï¼Œéœ€è¦é¢å¤–å­˜å‚¨`l`ï¼š$O(N)$
### IO
- è¯»`Q`ï¼š$O(Nd)$
- è¯»`Kã€V`ï¼š$T_{c} * O(Nd)$ = $O(T_{c}Nd)$
- å†™`O`ï¼š$O(Nd)$(`output`å’Œ`Q`çš„`shape`æ˜¯ä¸€æ ·çš„ï¼Œæ‰€ä»¥è¯»`Q`å’Œå†™`O`çš„`IO`æ¬¡æ•°æ˜¯ä¸€æ ·çš„)
- æœ€ç›´è§‚çš„å°±æ˜¯å†™`O`çš„æ¬¡æ•°å°‘äº†ï¼šä»$O(\frac{4N^2d^2}{M})$é™åˆ°äº†$O(Nd)$
### Implement
- åœ¨`flash.cu`ä¸­çš„`myForward_kernel2`
- æ¯ä¸ª`block`è´Ÿè´£ä¸€ä¸ªä¸€å°å—çš„ç»“æœï¼Œä¸€ä¸ªçº¿ç¨‹è´Ÿè´£ä¸€è¡Œçš„`Q`å’Œ`output`
![](imgs/flash-attention-2.png)


## æ—¶é—´æ¯”è¾ƒ
|    | Shape | CPU |  CUDA(3090)  |
|--------|-----|----------|----------|
| Standard   | [16, 12, 64, 64]  | 1.288s | 1.288s |
| Standard   | [16, 12, 256, 64]  | 1.664ms | 1.664ms |
| Standard   | [16, 12, 2048, 64]  | 38.963ms | 38.986ms |
| Flash 1   | [16, 12, 64, 64]  | 1.865ms | 1.288s |
| Flash 1   | [16, 12, 256, 64]  | 1.664ms | 735.000us |
| Flash 1   | [16, 12, 2048, 64]  | 19.276ms | 1.439ms |
| Flash 2   | [8, 12, 64, 64]  | 1.454ms | 602.000us |

## æ€»ç»“
- flash attention1ä¸­çš„on chip memoryæ²¡æœ‰$O_{B_{r}Ã—d}$
- flash attention2ä¸­ä¸è¦åœ¨HBMä¸Šå¼€$m_{BÃ—nhÃ—N}$
